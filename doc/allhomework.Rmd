---
title: "All homework"
author: "Hu Zongqing"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{All homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Question

5.4 Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf,
and use the function to estimate $F(x)$ for $x = 0.1, 0.2, . . . , 0.9$. Compare the
estimates with the values returned by the pbeta function in R.

5.9 The Rayleigh density [156,(18.76)] is
$$
f(x)=\frac{x}{\sigma^{2}} e^{-x^{2} /\left(2 \sigma^{2}\right)}, \quad x \geq 0, \sigma>0
$$
Implement a function to generate samples from a Rayleigh $(\sigma)$ distribution, using antithetic variables. What is the percent reduction in variance of $\frac{X+X^{\prime}}{2}$ compared with $\frac{X_{1}+X_{2}}{2}$ for independent $X_{1}, X_{2} ?$

5.13 Find two importance functions $f_{1}$ and $f_{2}$ that are supported on $(1, \infty)$ and are 'close' to
$$
g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1
$$
Which of your two importance functions should produce the smaller variance in estimating
$$
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x
$$
by importance sampling? Explain.

5.14 Obtain a Monte Carlo estimate of
$$
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x
$$
by importance sampling.

## Answer

5.4 

```{r}
set.seed(1234)
f <- function(x,n=100){##The original is larger.
u <- runif(n,0,x)
est <- x*mean(dbeta(u,3,3))
est
}
x <- 1:9/10
esti <- sapply(x,f)
theo <-pbeta(x,3,3)
cbind(esti,theo)
```

5.9
$$F^{-1}(x)=\sqrt{-2\sigma^2\log(1-x)}.$$
```{r}
n <- 100##The original is larger.
sigma <- 1
u1 <- runif(n)
x1 <- sqrt(-2*sigma^2*log(1-u1))
x2 <- sqrt(-2*sigma^2*log(u1))
u2 <- runif(n)
x3 <- sqrt(-2*sigma^2*log(1-u2))
(sd((x1+x3)/2)^2-sd((x1+x2)/2)^2)/sd((x1+x3)/2)^2
```

5.13\&5.14
$$f_1(x)=\frac{c_1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}I_{(x>1)},\quad c_1=\frac{1}{P(X>1)},X\sim N(0,1).$$
$$f_2(x)=c_2e^{-x}I_{(x>1)},\quad c=\frac{1}{P(X>1)},X\sim Exp(1).$$
```{r}
c1 <- 1/(1-pnorm(1))
c2 <- 1/(1-pexp(1))
n <- 100##The original is larger.
u1 <- rnorm(n,0,1)
x1 <- u1[which(u1>1)]
y1 <- 1/c1*x1^2
est1 <- mean(y1)
u2 <- rexp(n,1)
x2 <- u2[which(u2>1)]
y2 <- 1/c2*x2^2/sqrt(2*pi)*exp(-x2^2/2+x2)
est2 <- mean(y2)
cbind(est1,est2)
cbind(sd(x1)^2,sd(x2)^2)
```
可以看出第一种方差更小.


## Question

6.5 Suppose a $95 \%$ symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the $t$-interval for random samples of $\chi^{2}(2)$ data with sample size n=20. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)

6.A Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^{2}(1)$, (ii) Uniform (0,2), and (iii) Exponential(rate=1). In each case, test $H_{0}: \mu=\mu_{0}$ vs $H_{0}: \mu \neq \mu_{0}$, where $\mu_{0}$ is the mean of $\chi^{2}(1)$, Uniform (0,2), and Exponential(1), respectively.

If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at 0.05 level.

What is the corresponding hypothesis test problem?

What test should we use? Z-test, two-sample t-test, paired-t
test or McNemar test? Why?

Please provide the least necessary information for hypothesis
testing.

## Answer

6.5(真实均值为2，真实方差为4)
```{r}
set.seed(1234)
m <- 100##The original is larger.
n <- 20
alpha <- 0.05
y1 <- numeric(m)
y2 <- numeric(m)
for(j in 1:m){
x <- rchisq(n,2)
a <- mean(x)-sd(x)/sqrt(n)*qt(1-alpha/2,df=n-1)
b <- mean(x)+sd(x)/sqrt(n)*qt(1-alpha/2,df=n-1)
y1[j] <- as.numeric(2>a & 2<b)
UCL <- (n-1) * var(x) / qchisq(alpha, df=n-1)
y2[j] <- as.numeric(4<UCL)
}
cbind(mean(y1),mean(y2))
```
可以看出均值的覆盖率高于方差的覆盖率，因此更稳健。

6.A 

首先题目给的3中分布均值均为1.

另外，又数理统计的知识，在正态总体$N(\mu,\sigma^2)$下检验
$$H_0:\mu=\mu_0\leftrightarrow H_1:\mu\neq\mu_0,$$
可以取检验统计量$T=\sqrt{n}(\bar X-\mu_0)/S$，当$|T|>t_{n-1}(\alpha/2)$时拒绝$H_0$.
```{r}
m <- 100##The original is larger.
n <- 20
alpha <- 0.05
y1 <- numeric(m)
y2 <- numeric(m)
y3 <- numeric(m)
for(j in 1:m){
x1 <- rchisq(n,1)
t1 <- sqrt(n)*(mean(x1)-1)/sd(x1)
y1[j] <- as.numeric(abs(t1)>qt(1-alpha/2,df=n-1))
x2 <- runif(n,0,2)
t2 <- sqrt(n)*(mean(x2)-1)/sd(x2)
y2[j] <- as.numeric(abs(t2)>qt(1-alpha/2,df=n-1))
x3 <- rexp(n,1)
t3 <- sqrt(n)*(mean(x3)-1)/sd(x3)
y3[j] <- as.numeric(abs(t3)>qt(1-alpha/2,df=n-1))
}
cbind(mean(y1),mean(y2),mean(y3))
```
可以看出即使分布不再是正态分布，进行t-检验的第一类错误仍然较低。

最后一题: 

(1)根据计算功效的方式是取定$\theta_1$，进行$m=10000$次试验，得到
$$\hat\pi(\theta_1)=\frac{1}{m}\sum_{j=1}^{m}I_j.$$
而由功效的定义，我们有$I_j\,\,i.i.d\sim Bernulli(\pi(\theta_1))$于是$\sum_{i=1}^{m}I_j\sim Bin(m,\pi(\theta_1))$.

从而问题为已知$m=10000$,$\alpha=0.05$,$X_1,X_2$独立,$X_1\sim\frac{1}{m}Bin(m,p_1),X_2\sim\frac{1}{m}Bin(m,p_2)$($p_1,p_2$未知).想要求水平为1-$\alpha$的检验:
$$H_0:p_1=p_2\leftrightarrow H_1:p_1\neq p_2.$$

(2)可以采用两样本t-检验，这里m足够大，由中心极限定理，此时$(mX_i-mp_i)/\sqrt{mp_i(1-p_i)}$可以近似认为服从正态分布$N(0,1)$.

(3)只有当我们知道了$p_1$或$p_2$的值才能进行这样的检验。

## Question

6.C Repeat Examples $6.8$ and $6.10$ for Mardia's multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are iid, the multivariate population skewness $\beta_{1, d}$ is defined by Mardia as
$$
\beta_{1, d}=E\left[(X-\mu)^{T} \Sigma^{-1}(Y-\mu)\right]^{3}
$$
Under normality, $\beta_{1, d}=0$. The multivariate skewness statistic is
$$
b_{1, d}=\frac{1}{n^{2}} \sum_{i, j=1}^{n}\left(\left(X_{i}-\bar{X}\right)^{T} \widehat{\Sigma}^{-1}\left(X_{j}-\bar{X}\right)\right)^{3}
$$
where $\hat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $b_{1, d}$ are significant. The asymptotic distribution of $n b_{1, d} / 6$ is chisquared with $d(d+1)(d+2) / 6$ degrees of freedom.

## Answer

```{r}
set.seed(123)
library(MASS)
alpha <- 0.05#6.8
d <- 2
mu <- c(1,1)
Sigma <- matrix(c(2,1,1,3),2,2)
n <- c(10,20,30,50,100)
cv <- qchisq(0.95,d*(d+1)*(d+2)/6)*6/n
stat <- function(x) {
  xm <- cbind(x[,1]-mean(x[,1]),x[,2]-mean(x[,2]))
  n <- nrow(x)
  sigma <- t(xm)%*%xm/n
  ins <- solve(sigma)
  s <- xm%*%ins%*%t(xm)
  y <- sum(s^3)/n^2
  y
}
p <- numeric(length(n))
m <- 50##The original is larger.
for (i in 1:length(n)) {
  sktests <- numeric(m)
  for(j in 1:m){
    x <- mvrnorm(n=n[i],mu,Sigma)
    y <- stat(x)
    sktests[j] <- as.numeric(stat(x)>=cv[i]) 
  }
  p[i] <- mean(sktests)
}
p
#6.10
alpha <- 0.1
n <- 30
m <- 50##The original is larger.
epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
#critical value for the skewness test
cv <- qchisq(0.95,d*(d+1)*(d+2)/6)*6/n
for (j in 1:N) { #for each epsilon
e <- epsilon[j]
sktests <- numeric(m)
gener <- function(c){
  mu <- c(1,2)
  Sigma1 <- matrix(c(10,0,0,10),2,2)
  Sigma2 <- matrix(c(1,0,0,1),2,2)
  y <- c*mvrnorm(n=1,mu=c(1,2),Sigma=Sigma1)+(1-c)*mvrnorm(n=1,mu=c(1,2),Sigma=Sigma2)
  y
}
for (i in 1:m) { #for each replicate
r <- sample(c(0, 1), replace = TRUE, size = n, prob = c(1-e, e))
x <- t(sapply(r,gener))
sktests[i] <- as.integer(abs(stat(x)) >= cv)
}
pwr[j] <- mean(sktests)
}
#plot power vs epsilon
plot(epsilon, pwr, type = "b",
xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
```

## Question

Exercises 7.7, 7.8, 7.9, and 7.B (pages 213, Statistical Computating with R).

## Answer

7.7
```{r}
set.seed(123)
library('bootstrap')
n <- nrow(scor)
T <- cov(scor)
T1 <- svd(T)$d
theta.hat <- T1[1]/(T1[1]+T1[2]+T1[3]+T1[4]+T1[5])
B <- 200##The original is larger.
n <- nrow(scor)
theta.b <- numeric(B)
for (b in 1:B){
    i <- sample(1:n, size = n, replace = TRUE)
    scor1 <- scor[i,]
    T <- cov(scor1)
    T1 <- svd(T)$d
    theta.b[b] <- T1[1]/(T1[1]+T1[2]+T1[3]+T1[4]+T1[5])
}
bias <- mean(theta.b) - theta.hat
print(bias)
print(se.theta.b <- sd(theta.b))
```

7.8
```{r}
theta.jack <- numeric(n)
for(i in 1:n){
    T <- cov(scor[-i,])
    T1 <- svd(T)$d
    theta.jack[i] <- T1[1]/(T1[1]+T1[2]+T1[3]+T1[4]+T1[5])}
bias <- (n - 1) * (mean(theta.jack) - theta.hat)
print(bias)

se <- sqrt((n-1) *
mean((theta.jack - mean(theta.jack))^2))
print(se)
```

7.9
```{r}
alpha <- c(.025, .975)
print(quantile(theta.b, alpha))
conf <- 0.95
zalpha <- qnorm(alpha)
z0 <- qnorm(sum(theta.b < theta.hat) / B)
L <- mean(theta.jack) - theta.jack
a <- sum(L^3)/(6 * sum(L^2)^1.5)
adj.alpha <- pnorm(z0 + (z0+zalpha)/(1-a*(z0+zalpha)))
limits <- quantile(theta.b, adj.alpha)
print(limits)
```

7.B
```{r}
B <- 200##The original is larger.
n <- 40##The original is larger.
sk <- function(x) {
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}

m <- 50##The original is larger.

rx1sktests <- numeric(m)
lx1sktests <- numeric(m)
rx2sktests <- numeric(m)
lx2sktests <- numeric(m)
for (j in 1:m) {
x1 <- rnorm(n)
x2 <- rchisq(n,5)
theta.b <- numeric(B)
for (b in 1:B){
    i <- sample(1:n, size = n, replace = TRUE)
    x11 <- x1[i]
    x21 <- x2[i]
}
theta.x1 <- mean(x11)
cv <- qnorm(.975, 0, sqrt(6/B))
rx1sktests[j] <- as.integer(sk(x11) >= cv )
lx1sktests[j] <- as.integer(sk(x11) <= -cv ) 
rx2sktests[j] <- as.integer(sk(x21) >= cv )
lx2sktests[j] <- as.integer(sk(x21) <= -cv )
}
r1p.reject <- mean(rx1sktests)
l1p.reject <- mean(lx1sktests)
r2p.reject <- mean(rx2sktests)
l2p.reject <- mean(lx2sktests)
cbind(l1p.reject,r1p.reject)
cbind(l2p.reject,r2p.reject)
```

## Question

8.2 Implement the bivariate Spearman rank correlation test for independence[255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples.

$\cdot$Design experiments for evaluating the performance of the NN, energy, and ball methods in various situations.

Unequal variances and equal expectations

Unequal variances and unequal expectations

Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normaldistributions)

Unbalanced samples (say, 1 case versus 10 controls)

Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).

## Answer

8.2
```{r}
library(boot)
set.seed(123)
n <- 50##The original is larger.
x <- rnorm(n,0,1)
y <- rexp(n,1)
z <- cbind(x,y)
cor2 <- function(a,ix,dims){
p <- dims[1]
q <- dims[2]
d <- p + q
x <- z[ , 1:p]
y <- z[ix, -(1:p)]
return(cor(x,y,method="spearman"))
}
boot.obj <- boot(data = z, statistic = cor2, R = 999, sim = "permutation",dims = c(1, 1))
tb <- c(boot.obj$t0, boot.obj$t)
pcor <- mean(tb>=tb[1])
prefer <- cor.test(x,y,method="spearman")$p.value
c(pcor,prefer)
```
$\cdot$
```{r}
n <- 50##The original is larger.
x1 <- rnorm(n,2,1)
y1 <- rexp(n,2)
z1 <- c(x1,y1)
x2 <- rnorm(n,1,1)
y2 <- rexp(n,2)
z2 <- c(x2,y2)
x3 <- rt(n,1)
y3 <- 0.5*rnorm(n,0,2)+0.5*rnorm(n,2,1)
z3 <- c(x3,y3)

library(RANN)
library(energy)
library(Ball)
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1)
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
N <- c(n, n)

boot.obj <- boot(data = z1, statistic = Tn, R = 999,
sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
N1p.value <- mean(ts>=ts[1])

boot.obs <- eqdist.etest(z1, sizes=N, R=999)
E1p.value <- boot.obs$p.value

B1p.value = bd.test(x = x1, y = y1, num.permutations=999)$p.value

c(N1p.value,E1p.value,B1p.value)

boot.obj <- boot(data = z2, statistic = Tn, R = 999,
sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
N2p.value <- mean(ts>=ts[1])

boot.obs <- eqdist.etest(z2, sizes=N, R=999)
E2p.value <- boot.obs$p.value

B2p.value = bd.test(x = x2, y = y2, num.permutations=999)$p.value

c(N2p.value,E2p.value,B2p.value)

boot.obj <- boot(data = z3, statistic = Tn, R = 999,
sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
N3p.value <- mean(ts>=ts[1])

boot.obs <- eqdist.etest(z3, sizes=N, R=999)
E3p.value <- boot.obs$p.value

B3p.value = bd.test(x = x3, y = y3, num.permutations=999)$p.value

c(N3p.value,E3p.value,B3p.value)
```

## Question

Exercies 9.3 and 9.8 (pages 277-278, Statistical Computating
with R).
I For each of the above exercise, use the Gelman-Rubin method
to monitor convergence of the chain, and run the chain until it
converges approximately to the target distribution according to
$\hat R < 1.2.$

## Answer

9.3
```{r}
set.seed(123)
f <- function(x, location = 0, scale = 1) {
stopifnot(scale > 0)
return(1/(scale*pi*(1+((x-location)/scale)^2)))
}
m <- 100##The original is larger.
x <- numeric(m)
x[1] <- rnorm(1, mean=1)
k <- 0
u <- runif(m)
for (i in 2:m) {
xt <- x[i-1]
y <- rnorm(1, mean = xt)
num <- f(y) * dnorm(xt, mean = y)
den <- f(xt) * dnorm(y, mean = xt)
if (u[i] <= num/den) x[i] <- y else {
x[i] <- xt
k <- k+1
}
}
bx <- x[-(1:10)]##The original is larger.
r <- 1:9/10
qx <- quantile(bx,r)
q <- qcauchy(r)
t(cbind(qx,q))
```
9.8
```{r}
N <- 500 #length of chain    ##The original is larger.
burn <- 100 #burn-in length  ##The original is larger.
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- -.75 #correlation
a <- 1
b <- 2
n <- 3
X[1, ] <- c(0,0.5) #initialize
for (i in 2:N) {
y1 <- X[i-1, 2]
X[i, 1] <- rbinom(n = 1, size = n, prob = y1)
x1 <- X[i, 1]
c1 <- x1+a
c2 <- n-x+b
X[i, 2] <- rbeta(1, c1, c2)
}
b <- burn + 1
x <- X[b:N, ]
```
use the Gelman-Rubin method:
```{r}
Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)
psi.means <- rowMeans(psi) #row means
B <- n * var(psi.means) #between variance est.
psi.w <- apply(psi, 1, "var") #within variances
W <- mean(psi.w) #within est.
v.hat <- W*(n-1)/n + (B/n) #upper variance est.
r.hat <- v.hat / W #G-R statistic
return(r.hat)
}
```
9.3
```{r}
cauchy.chain <- function(N,X1){
f <- function(x, location = 0, scale = 1) {
stopifnot(scale > 0)
return(1/(scale*pi*(1+((x-location)/scale)^2)))
}
m <- N
x <- numeric(m)
x[1] <- X1
k <- 0
u <- runif(m)
for (i in 2:m) {
xt <- x[i-1]
y <- dnorm(1, mean = xt)
num <- f(y) * dnorm(xt, mean = y)
den <- f(xt) * dnorm(y, mean = xt)
if (u[i] <= num/den) x[i] <- y else {
x[i] <- xt
k <- k+1
}
}
return(x)
}
k <- 4
n <- 1500##The original is larger.
b <- 100##The original is larger.
x0 <- c(-0.75,-0.25,0.25,0.75)
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
X[i, ] <- cauchy.chain(n, x0[i])
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
#plot psi for the four chains
#par(mfrow=c(2,2))
for (i in 1:k)
plot(psi[i, (b+1):n], type="l",
xlab=i, ylab=bquote(psi))
#par(mfrow=c(1,1)) #restore default
#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in (b+1):n)
rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```

## Question

Exercises 11.3 and 11.5 (pages 353-354, Statistical Computing
with R)

Suppose $T_1, . . . ,T_n$ are i.i.d. samples drawn from the
exponential distribution with expectation $\lambda$. Those values
greater than $\tau$ are not observed due to right censorship, so that
the observed values are $Y_i = T_i I(Ti \le \tau ) + \tau I(T_i > \tau)$,
i = 1, . . . , n. Suppose $\tau = 1$ and the observed $Y_i$ values are as
follows:
0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85
Use the E-M algorithm to estimate $\lambda$, compare your result with
the observed data MLE (note: $Y_i$ follows a mixture
distribution).

## Answer

11.3
```{r}
f <- function(k,a){
  d <- length(a)
  l <- (k+1)*log(sum(a*a))-lgamma(k+1)-k*log(2)-log(2*k+1)-log(2*k+2)+lgamma((d+1)/2)+lgamma(k+1.5)-lgamma(k+d/2+1)
  exp(l)*(-1)^k
}

a <- c(1:20)
k <- 100
f(k,a)

S <- function(a,n=100){#the amount of terms calculated  ##The original is larger.
  sum(sapply(n:0,function(k) f(k,a)))
}

a <- c(1,2)
S(a)
```
11.5
```{r}
k <- c(4,25)
m <- length(k)
asolve <- numeric(m)

f <- function(k,a){
  lgamma(k/2)-lgamma((k-1)/2)-log(k-1)/2+log(integrate(function(u) (1+u^2/(k-1))^(-k/2),lower=0,upper=sqrt(a^2*(k-1)/(k-a^2)))$value)-lgamma((k+1)/2)+lgamma((k)/2)+log(k)/2-log(integrate(function(u) (1+u^2/(k))^(-(k+1)/2),lower=0,upper=sqrt(a^2*(k)/(k+1-a^2)))$value)
}

for(i in 1:m){
  asolve[i] <- uniroot(function(a) f(k[i],a),c(0.1,k[i]^0.5-0.01))$root
}

cbind(k,asolve)
```
exercise
假设完全数据为$t_1,\cdots,t_n$
$$f(y,t|\lambda)=\frac{1}{\lambda^n}e^{-\frac{\sum t_i}{\lambda}}.$$
$$l(\lambda|y,t)=-n\log\lambda-\frac{1}{\lambda}\sum_{y_i<1}y_i-\frac{1}{\lambda}\sum_{y_i=1}t_iI(t_i\ge1).$$
E-step:
$$
E_{\hat\lambda_0}[l(\lambda|y,t)|y,\lambda]=-n\log\lambda-\frac{1}{\lambda}\sum_{y_i<1}y_i-\frac{1}{\lambda}\sum_{y_i=1}E_{\hat\lambda_0}t_iI(t_i\ge1)
$$
$$
\begin{aligned}
&E_{\hat\lambda_0}t_iI(t_i\ge1)=\int_1^{+\infty}t\cdot\frac{c_0}{\hat\lambda_0}e^{-\frac{t}{\hat\lambda_0}}dt\quad(c_0=\frac{1}{P_{\hat\lambda_0}(T\ge1)}=e^{\frac{1}{\hat\lambda_0}})\\
&=c_0\hat\lambda_0\int_{\frac{1}{\hat\lambda_0}}^{+\infty}te^{-t}dt=c_0\hat\lambda_0(-(t+1)e^{-t})\Big|_{\frac{1}{\hat\lambda_0}}^{+\infty}\\
&=c_0\hat\lambda_0(\frac{1}{\hat\lambda_0}+1)e^{-\frac{1}{\hat\lambda_0}}=\hat\lambda_0+1.
\end{aligned}
$$
从而
$$E_{\hat\lambda_0}[l(\lambda|y,t)|y,\lambda]=-n\log\lambda-\frac{1}{\lambda}\sum_{y_i<1}y_i-\frac{1}{\lambda}(\hat\lambda_0+1)\sum I(y_i=1).$$
M-step:$\quad\frac{\partial E_{\hat\lambda_0}[l(\lambda|y,t)|y,\lambda]}{\partial\lambda}=0.$
$$\hat\lambda_1=\frac{1}{n}\Big(\sum_{y_i<1}y_i+(\hat\lambda_0+1)\sum I(y_i=1)\Big).$$
收敛时满足:
$$\lambda=\frac{1}{n}\Big(\sum_{y_i<1}y_i+(\lambda+1)\sum I(y_i=1)\Big)\quad\hat\lambda_{EM}=\frac{\sum\limits_{y_i<1}y_i+\sum I(y_i=1)}{n-\sum I(y_i=1)}.$$
```{r}
y <- c(0.54,0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85)
print(sum(y)/(length(y)-sum(y[which(y==1)])))
```
$$f(y|\lambda)=\frac{1}{\lambda^{n-\sum I(y_i=1)}}e^{-\frac{1}{\lambda}\sum\limits_{y_i<1}y_i}\cdot e^{-\frac{1}{\lambda}\sum\limits_{y_i=1}1}.$$
$$l(\lambda|y)=-(n-\sum I(y_i=1))\log\lambda-\frac{1}{\lambda}\sum_{y_i<1}y_i-\frac{1}{\lambda}\sum_{y_i=1}1.$$
$$\frac{\partial l(\lambda|y)}{\partial \lambda}=0.\quad \hat\lambda_{MLE}=\frac{\sum\limits_{y_i<1}y_i+\sum\limits_{y_i=1}1}{n-\sum I(y_i=1)}.$$
$$\hat\lambda_{EM}=\hat\lambda_{MLE}.$$

## Question

Exercises 1 and 5 (page 204, Advanced R)

Excecises 1 and 7 (page 214, Advanced R)

## Answer

P204EX1

函数lapply(X, FUN, ...)中...可以输入FUN中的量，第二种表述也就是在mean函数中的变量x 取成上面生成的数据x，与第一种等价。

P204EX5
EX3
```{r}
rsq <- function(mod) summary(mod)$r.squared
data <- mtcars
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
modl <- lapply(formulas,lm,data = data)
r2list <- lapply(modl,rsq)
r2list
```
EX4
```{r}
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
modl <- lapply(bootstraps,lm,formula = mpg ~ disp)
r2list_lapply <- lapply(modl,rsq)
r2list_lapply

n <- length(bootstraps)
modl <- vector("list", n)
for (i in 1:n) {
modl[[i]] <- lm(formula = mpg ~ disp,data = bootstraps[[i]])
}
r2list_for <- lapply(modl,rsq)
r2list_for
```

P214EX1
```{r}
data <- mtcars
vapply(1:ncol(data),function(i) sd(data[,i]),FUN.VALUE=0)
data <- InsectSprays
vapply(1:ncol(data),function(i) if(typeof(data[,i])=="double")sd(data[,i]) else 0,FUN.VALUE=0)
```

P214EX7
```{r}
library("parallel")
mcsapply <- function(mc.cores,X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE){
  cl <- makeCluster(getOption("cl.cores", mc.cores))
  y <- parSapply(cl, X, FUN, ..., simplify = TRUE,
          USE.NAMES = TRUE, chunk.size = NULL)
  stopCluster(cl)
  y
}

mcvapply <- function(mc.cores,X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE){
  cl <- makeCluster(getOption("cl.cores", mc.cores))
  y <- parSapply(cl, X, FUN, ..., simplify = TRUE,
          USE.NAMES = TRUE, chunk.size = NULL)
  a <- which(typeof(y)!=typeof(FUN.VALUE))
  y[a] <- rep(NA,length(a))
  stopCluster(cl)
  y
}
```

## Answer

采用$a=1,b=2,n=3$

```{r}
library("StatComp21013")
suppressWarnings(library('Rcpp'))
suppressWarnings(library('microbenchmark'))
#source("gibbsR.R")
#sourceCpp("gibbsC.cpp")
gibbR <- gibbsR(100,10)
gibbC <- gibbsC(100,10)
xR <- gibbR[,1]
yR <- gibbR[,2]
xC <- gibbC[,1]
yC <- gibbC[,2]
qqplot(xR,xC)
qqplot(yR,yC)
ts <- microbenchmark(gibbR=gibbsR(100,10),gibbC=gibbsC(100,10))
summary(ts)[,c(1,3,5,6)]
```

comments: 比较qq图发现，在排除因种子不同导致的微小差距之后，两种方法产生的随机数一致，但C运行的更快。
